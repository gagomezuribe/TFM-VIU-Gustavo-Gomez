{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPGVES5MzwcYZWJOm83QniX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\" MODELO PRE ENTRENADO VIT base\n",
        "Dataset: Patch Camelyon\n",
        "Fuente del modelo: HuggingFace\n",
        "Modelo: google/vit-base-patch16-224-in21k\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "hU9YVSAQZ3Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conectar con Google Drive y HuggingFace\n",
        "from google.colab import drive\n",
        "_ = drive.mount('/content/drive')\n",
        "base_folder = \"/content/drive/MyDrive/00 VIU/10 TFM\"\n",
        "\n",
        "from google.colab import userdata\n",
        "userdata.get('WANDB_API_KEY')"
      ],
      "metadata": {
        "id": "zV_M0oY3IVos",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar las librerías necesarias\n",
        "!pip install tf-keras\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade evaluate\n",
        "!pip install -q tensorflow"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IJnZxj_wZapy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqsB75hSYZol"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from transformers import AutoModelForImageClassification\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer\n",
        "import evaluate\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase para convertir el dataset de tensorflow en un dataset de torch\n",
        "class TfToTorchvisionDataset(Dataset):\n",
        "    def __init__(self, tf_dataset):\n",
        "        self.data = list(tf_dataset)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_item = self.data[idx]\n",
        "        image = torch.from_numpy(data_item['image'].numpy())\n",
        "        image = image.permute(2,0,1)\n",
        "        label = torch.tensor(data_item['label'].numpy())\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "Q9sNftrfINH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar en memoria los datasets de training y val:\n",
        "ruta_pcam_tr = base_folder+\"/Datasets/pcamelyon_tf/TFcamelyon_train\"\n",
        "ds_tr = tf.data.Dataset.load(ruta_pcam_tr)\n",
        "\n",
        "torch_dataset_tr = TfToTorchvisionDataset(ds_tr)\n",
        "ds_tr=None\n",
        "\n",
        "ruta_pcam_val = base_folder+\"/Datasets/pcamelyon_tf/TFcamelyon_val\"\n",
        "ds_val = tf.data.Dataset.load(ruta_pcam_val)\n",
        "torch_dataset_val = TfToTorchvisionDataset(ds_val)\n",
        "ds_val=None"
      ],
      "metadata": {
        "id": "CirSNlu3INLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar el modelo pre-entrenado\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "# Definir el número de clases\n",
        "num_labels = 2\n"
      ],
      "metadata": {
        "id": "6eROzIfZYdr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar el procesador de imágenes\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_name,do_rescale=True)"
      ],
      "metadata": {
        "id": "6Xqo08KqYdx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch_dataset_tr\n",
        "val_dataset = torch_dataset_val"
      ],
      "metadata": {
        "id": "L52l6FCWYd3V",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de ensamble y preparción de lotes para el Trainer\n",
        "def collate_fn(batch):\n",
        "    images = [to_pil_image(item[0]) for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
        "    labels = torch.tensor(labels,dtype=torch.long)\n",
        "    return {\"pixel_values\": inputs[\"pixel_values\"], \"labels\": labels}"
      ],
      "metadata": {
        "id": "n-lrxKBkBujr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar el modelo pre-entrenado para clasificación de imágenes\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "               model_name,num_labels=num_labels, ignore_mismatched_sizes=True)"
      ],
      "metadata": {
        "id": "Z5KHkihyYeG9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir los argumentos de entrenamiento en la clase training_args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=base_folder+\"/Modelos entrenados/ViT base v1\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "nRAybmWSYeJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definir la métrica (exactitud)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    predictions = np.argmax(p.predictions, axis=1)\n",
        "    references = p.label_ids\n",
        "    metrics_result = metric.compute(predictions=predictions, references=references)\n",
        "    print(f\"Metric computation result: {metrics_result}\")\n",
        "    return {\"eval_accuracy\": metrics_result[\"accuracy\"]}"
      ],
      "metadata": {
        "id": "n5iouSV6YeMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Instanciar el Trainer:\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "id": "fFSbKp_CY3fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Número de ejemplos de entrenamiento: {len(train_dataset)}\")\n",
        "print(f\"Número de ejemplos de prueba: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "id": "hbyekhmnrayS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar el fine-tuning\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "pupwhPHpY3i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUACION DEL MODELO\n",
        "\n",
        "# Cargar el Dataset de test:\n",
        "ruta_pcam_test = base_folder+\"/Datasets/pcamelyon_tf/TFcamelyon_test\"\n",
        "ds_test = tf.data.Dataset.load(ruta_pcam_test)\n",
        "torch_dataset_test = TfToTorchvisionDataset(ds_test)\n",
        "ds_test = None\n",
        "\n",
        "print(f\"Número de ejemplos de test: {len(torch_dataset_test)}\")\n",
        "\n",
        "# Evaluar el modelo en el conjunto de test\n",
        "eval_results = trainer.evaluate(eval_dataset=torch_dataset_test)\n",
        "\n",
        "print(\"Resultados de la evaluación en el conjunto de test:\")\n",
        "eval_results\n"
      ],
      "metadata": {
        "id": "ieUQWSRlY3mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo y los pesos\n",
        "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "processor.save_pretrained(base_folder+\"/Modelos entrenados/google-vit-base v1\")\n",
        "model.save_pretrained(base_folder+\"/Modelos entrenados/google-vit-base v1\")"
      ],
      "metadata": {
        "id": "u7DPsTf-mYdk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir nuevos argumentos de entrenamiento para una época adicional\n",
        "additional_training_args = TrainingArguments(\n",
        "    output_dir=base_folder+\"/Modelos entrenados/ViT base v1_epoca2\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Instanciar el nuevo Trainer con los pesos actuales y los nuevos argumentos\n",
        "trainer_additional_epoch = Trainer(\n",
        "    model=model,\n",
        "    args=additional_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_additional_epoch.train()"
      ],
      "metadata": {
        "id": "ZjBn8BjNT7At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ruta_pcam_test = base_folder+\"/Datasets/pcamelyon_tf/TFcamelyon_test\"\n",
        "ds_test = tf.data.Dataset.load(ruta_pcam_test)\n",
        "torch_dataset_test = TfToTorchvisionDataset(ds_test)\n",
        "ds_test = None\n",
        "\n",
        "print(f\"Número de ejemplos de test: {len(torch_dataset_test)}\")\n",
        "\n",
        "# Evaluar el modelo en el conjunto de test\n",
        "eval_results = trainer.evaluate(eval_dataset=torch_dataset_test)\n",
        "\n",
        "print(\"Resultados de la evaluación en el conjunto de test:\")\n",
        "eval_results\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "eKHaNenAZ6Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo y los pesos\n",
        "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "processor.save_pretrained(base_folder+\"/Modelos entrenados/google-vit-base v1\")\n",
        "model.save_pretrained(base_folder+\"/Modelos entrenados/google-vit-base v1\")"
      ],
      "metadata": {
        "id": "_uY7lzAJZ6Uf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}