# -*- coding: utf-8 -*-
"""App gradio huggingface 160725 0619.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u3gIxUlRnfrgtWUrlKe9YxuuY1gJXB7u

Corrección al error de getthumbnail de slide propuesta por gemini
"""

import gradio as gr
import openslide
import os
import datetime
import torch
import pandas as pd
import numpy as np
import requests
from transformers import AutoModelForImageClassification, AutoImageProcessor
import matplotlib.pyplot as plt

model_path = "gagomezuribe/Swin_Base_v1_230625" #modelo cargado en huggingface
print(f"DEBUG: Modelo a cargar: {model_path}")
try:
    image_processor = AutoImageProcessor.from_pretrained(model_path)
    num_labels = 2
    model = AutoModelForImageClassification.from_pretrained(
        model_path,
        num_labels=num_labels,
        ignore_mismatched_sizes=True
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    print(f"DEBUG: Modelo '{model_path}' cargado exitosamente en {device}.")
except Exception as e:
    print(f"ERROR: No se pudo cargar el modelo o procesador: {e}")
    image_processor = None
    model = None
    device = "cpu"

LOCAL_DOWNLOAD_DIR = "./downloaded_wsi_cache" # directorio para la copia temporal
os.makedirs(LOCAL_DOWNLOAD_DIR, exist_ok=True)
print(f"DEBUG: Directorio de descarga local: {os.path.abspath(LOCAL_DOWNLOAD_DIR)}")

def analizar_slide_con_presigned_url(presigned_url, nombre_carpeta_salida, progress=gr.Progress()):
    if model is None or image_processor is None:
        return "Error: El modelo no se pudo cargar al inicio del Space.", "", None, None, None

    patch_num, fila, col, cant_in_batch = 0, 0, 0, 0
    lista_imgs, heatmap, list_filas, list_columnas, list_parches = [], [], [], [], []

    # Extraer el nombre del archivo a partir de la URL y preparar el path local:
    filename = os.path.basename(presigned_url.split('?')[0])
    local_wsi_path = os.path.join(LOCAL_DOWNLOAD_DIR, filename)

    progress(0, desc=f"Iniciando descarga de '{filename}' usando la URL pública...")

    try:
        response = requests.get(presigned_url, stream=True)
        response.raise_for_status() # Lanza una excepción para errores HTTP (4xx o 5xx)

        total_size = int(response.headers.get('content-length', 0))
        downloaded_size = 0

        with open(local_wsi_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192): # Descargar en chunks
                f.write(chunk)
                downloaded_size += len(chunk)
                current_progress = downloaded_size / total_size if total_size > 0 else 0
                progress(0.05*current_progress, desc=f"Descargando {filename}...")

        print(f"DEBUG: Archivo '{filename}' descargado exitosamente a '{local_wsi_path}'.")
        progress(0.15, desc=f"Descarga de '{filename}' completada. Analizando...") # Progreso hasta 0.15 para descarga

    except requests.exceptions.RequestException as e:
        return f"Error de red/descarga de la URL pre-firmada: {e}. Asegúrate de que la URL sea válida y no haya expirado.", "", None, None, None
    except Exception as e:
        return f"Error inesperado durante la descarga: {e}", "", None, None, None

    try:
        with openslide.OpenSlide(local_wsi_path) as slide:
            WSI_w, WSI_h = slide.level_dimensions[0]
            p_size = 224
            batch_p = 32
            step_size_level0 = p_size * int(slide.level_downsamples[1]) # Asumiendo nivel 1 es 20x (factor 2x)

            start_time = datetime.datetime.now()

            # Bucle de inferencia:
            for fila_level0 in range(0, WSI_h, step_size_level0):
                for col_level0 in range(0, WSI_w, step_size_level0):
                    current_patch_w_level0 = min(step_size_level0, WSI_w - col_level0)
                    current_patch_h_level0 = min(step_size_level0, WSI_h - fila_level0)

                    if current_patch_w_level0 >= step_size_level0 and current_patch_h_level0 >= step_size_level0:
                        img = slide.read_region((col_level0, fila_level0), 1, (p_size, p_size))
                        lista_imgs.append(img.convert('RGB'))
                        list_filas.append(fila_level0)
                        list_columnas.append(col_level0)
                        list_parches.append(patch_num)
                        cant_in_batch += 1

                        if cant_in_batch == batch_p:
                            inputs = image_processor(images=lista_imgs, return_tensors="pt")
                            inputs = {k: v.to(device) for k, v in inputs.items()}
                            with torch.no_grad():
                                outputs = model(pixel_values=inputs['pixel_values'])
                                logits = outputs.logits
                                probs = torch.softmax(logits, dim=-1)
                                listado = [float(probs[i, 1]) for i in range(probs.shape[0])]
                                heatmap.extend(listado)
                            lista_imgs = []
                            cant_in_batch = 0
                    patch_num += 1


                current_analysis_progress = fila_level0 / WSI_h
                overall_progress = 0.05 + 0.95 * current_analysis_progress
                progress(overall_progress, desc="Avance del análisis...")

                if 'estimated_total_minutes' not in locals() and (datetime.datetime.now() - start_time).total_seconds() > 60 and current_analysis_progress > 0:
                    t_elapsed = (datetime.datetime.now() - start_time).total_seconds() / 60
                    estimated_total_minutes = t_elapsed / current_analysis_progress
                    progress(overall_progress, desc=f"Minutos estimados totales (solo análisis): {int(estimated_total_minutes)}")


            if cant_in_batch > 0: # Procesar lote restante
                inputs = image_processor(images=lista_imgs, return_tensors="pt")
                inputs = {k: v.to(device) for k, v in inputs.items()}
                with torch.no_grad():
                    outputs = model(pixel_values=inputs['pixel_values'])
                    logits = outputs.logits
                    probs = torch.softmax(logits, dim=-1)
                    listado = [float(probs[i, 1]) for i in range(probs.shape[0])]
                    heatmap.extend(listado)

            output_base_dir = "output_files"
            full_path_to_folder = os.path.join(output_base_dir, nombre_carpeta_salida)
            os.makedirs(full_path_to_folder, exist_ok=True)

            patches_per_row = WSI_w // step_size_level0
            if len(heatmap) % patches_per_row != 0:
                padding_needed = patches_per_row - (len(heatmap) % patches_per_row)
                heatmap.extend([0.0] * padding_needed)

            heatmap_np = np.array(heatmap).reshape(-1, patches_per_row)
            df = pd.DataFrame(heatmap_np)
            excel_path = os.path.join(full_path_to_folder, 'heatmap.xlsx')
            df.to_excel(excel_path)

            thumb_img = slide.get_thumbnail((1024, 1024))
            thumb_png_path = os.path.join(full_path_to_folder, "thumb.png")
            thumb_img.save(thumb_png_path)

            plt.figure(figsize=(8, 6))
            im = plt.imshow(heatmap_np, cmap='RdYlGn_r', interpolation='nearest', origin="upper", vmin=0, vmax=1)
            plt.title(f"Mapa de Calor de Metástasis para {filename}")
            plt.colorbar(im, label="Probabilidad de Metástasis")
            plt.axis('off')
            heatmap_png_path = os.path.join(full_path_to_folder, "heatmap.png")
            plt.savefig(heatmap_png_path, bbox_inches='tight', pad_inches=0)
            plt.close()

            data_for_hist = np.array(heatmap)
            bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
            counts, _ = np.histogram(data_for_hist, bins=bins)
            plt.figure(figsize=(7, 5))
            plt.bar(bins[:-1], counts, width=0.15, align='edge', edgecolor='black')
            plt.title("Distribución de Probabilidades de Metástasis")
            plt.xlabel("Probabilidad")
            plt.ylabel("Frecuencia de Parches")
            plt.xticks(bins)
            buckets_png_path = os.path.join(full_path_to_folder, "buckets.png")
            plt.savefig(buckets_png_path, bbox_inches='tight', pad_inches=0)
            plt.close()

            info_text = f"Análisis de '{filename}' finalizado con normalidad.\n"
            info_text += f"Archivos de resultados en: {os.path.abspath(full_path_to_folder)}\n"
            info_text += "Excel, Miniatura, Mapa de Calor y Distribución creados."

            return info_text, "Análisis completado.", excel_path, heatmap_png_path, buckets_png_path

    except openslide.OpenSlideError as e:
        return f"Error de OpenSlide: {e}. El archivo puede no ser un formato WSI compatible o estar corrupto.", "Fallo", None, None, None
    except Exception as e:
        return f"Error durante el análisis del WSI: {e}", "Fallo", None, None, None
    finally:
        # Limpiar el archivo descargado para liberar almacenamiento temporal
        if os.path.exists(local_wsi_path):
            os.remove(local_wsi_path)
            print(f"DEBUG: Archivo temporal '{local_wsi_path}' eliminado.")

# Interfaz de Gradio:
demo = gr.Interface(
    fn=analizar_slide_con_presigned_url,
    inputs=[
        gr.Textbox(label="URL pública del archivo WSI en la Nube"),
        gr.Textbox(label="Nombre de la carpeta para guardar resultados (ej: resultados_biopsia_x)",
                   info="Se creará en un subdirectorio 'output_files' del Space.")
    ],
    outputs=[
        gr.Textbox(label="Información General", lines=4),
        gr.Textbox(label="Estado de la Ejecución"),
        gr.File(label="Descargar Excel de Heatmap", type="filepath"),
        gr.Image(label="Mapa de Calor Visual", type="filepath"),
        gr.Image(label="Diagrama de Distribución", type="filepath")
    ],
    title="Análisis de Metástasis en Biopsias WSI (via URL Pre-Firmada)",
    description="Indicar la URL pública donde se encuentra el WSI para analizarlo.\
    El modelo extraerá parches, realizará inferencia y generará un mapa de calor y un histograma de probabilidades. \
    La ejecución puede tardar hasta un par de horas para WSIs grandes.",
    flagging_mode="never",
)

if __name__ == "__main__":
    demo.launch(share=False)