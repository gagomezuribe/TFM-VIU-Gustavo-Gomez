# -*- coding: utf-8 -*-
"""GoogleViT-base v1 220625.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TSu9vDkNiXYazu9xsB21h9t0YJcKg8-H

MODELO PRE ENTRENADO VIT base
Dataset: Patch Camelyon
Fuente del modelo: HuggingFace
Modelo: google/vit-base-patch16-224-in21k
"""

# Conectar con Google Drive y HuggingFace
from google.colab import drive
_ = drive.mount('/content/drive')
base_folder = "/content/drive/MyDrive/00 VIU/10 TFM"

from google.colab import userdata
userdata.get('WANDB_API_KEY')

# Instalar las librerías necesarias
!pip install tf-keras
!pip install --upgrade transformers
!pip install --upgrade evaluate
!pip install -q tensorflow

import numpy as np
import io
import os

import torch
from torch.utils.data import Dataset
from torchvision.transforms.functional import to_pil_image

import tensorflow as tf
import tensorflow_datasets as tfds

from transformers import AutoModelForImageClassification
from transformers import AutoImageProcessor
from transformers import TrainingArguments
from transformers import Trainer
import evaluate

from PIL import Image

# Clase para convertir el dataset de tensorflow en un dataset de torch
class TfToTorchvisionDataset(Dataset):
    def __init__(self, tf_dataset):
        self.data = list(tf_dataset)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        data_item = self.data[idx]
        image = torch.from_numpy(data_item['image'].numpy())
        image = image.permute(2,0,1)
        label = torch.tensor(data_item['label'].numpy())
        return image, label



# Cargar en memoria los datasets de training y val:
ruta_pcam_tr = base_folder+"/Datasets/pcamelyon_tf/TFcamelyon_train"
ds_tr = tf.data.Dataset.load(ruta_pcam_tr)

torch_dataset_tr = TfToTorchvisionDataset(ds_tr)
ds_tr=None

ruta_pcam_val = base_folder+"/Datasets/pcamelyon_tf/TFcamelyon_val"
ds_val = tf.data.Dataset.load(ruta_pcam_val)
torch_dataset_val = TfToTorchvisionDataset(ds_val)
ds_val=None

# Instanciar el modelo pre-entrenado
model_name = "google/vit-base-patch16-224-in21k"
# Definir el número de clases
num_labels = 2

# Instanciar el procesador de imágenes
image_processor = AutoImageProcessor.from_pretrained(model_name,do_rescale=True)

train_dataset = torch_dataset_tr
val_dataset = torch_dataset_val

# Función de ensamble y preparción de lotes para el Trainer
def collate_fn(batch):
    images = [to_pil_image(item[0]) for item in batch]
    labels = [item[1] for item in batch]
    inputs = image_processor(images=images, return_tensors="pt")
    labels = torch.tensor(labels,dtype=torch.long)
    return {"pixel_values": inputs["pixel_values"], "labels": labels}

# Instanciar el modelo pre-entrenado para clasificación de imágenes
model = AutoModelForImageClassification.from_pretrained(
               model_name,num_labels=num_labels, ignore_mismatched_sizes=True)

# Definir los argumentos de entrenamiento en la clase training_args
training_args = TrainingArguments(
    output_dir=base_folder+"/Modelos entrenados/ViT base v1",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=1,
    learning_rate=2e-5,
    num_train_epochs=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    logging_dir="./logs",
    logging_steps=100,
    remove_unused_columns=False,
    report_to="none",
)

#Definir la métrica (exactitud)
metric = evaluate.load("accuracy")
def compute_metrics(p):
    predictions = np.argmax(p.predictions, axis=1)
    references = p.label_ids
    metrics_result = metric.compute(predictions=predictions, references=references)
    print(f"Metric computation result: {metrics_result}")
    return {"eval_accuracy": metrics_result["accuracy"]}

# Instanciar el Trainer:
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collate_fn,
    compute_metrics=compute_metrics
)

print(f"Número de ejemplos de entrenamiento: {len(train_dataset)}")
print(f"Número de ejemplos de prueba: {len(val_dataset)}")

# Ejecutar el fine-tuning
trainer.train()

# EVALUACION DEL MODELO

# Cargar el Dataset de test:
ruta_pcam_test = base_folder+"/Datasets/pcamelyon_tf/TFcamelyon_test"
ds_test = tf.data.Dataset.load(ruta_pcam_test)
torch_dataset_test = TfToTorchvisionDataset(ds_test)
ds_test = None

print(f"Número de ejemplos de test: {len(torch_dataset_test)}")

# Evaluar el modelo en el conjunto de test
eval_results = trainer.evaluate(eval_dataset=torch_dataset_test)

print("Resultados de la evaluación en el conjunto de test:")
eval_results

# Guardar el modelo y los pesos
processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
processor.save_pretrained(base_folder+"/Modelos entrenados/google-vit-base v1")
model.save_pretrained(base_folder+"/Modelos entrenados/google-vit-base v1")

# Definir nuevos argumentos de entrenamiento para una época adicional
additional_training_args = TrainingArguments(
    output_dir=base_folder+"/Modelos entrenados/ViT base v1_epoca2",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=1,
    learning_rate=2e-5,
    num_train_epochs=1,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    logging_dir="./logs",
    logging_steps=10,
    remove_unused_columns=False,
    report_to="none",
)

# Instanciar el nuevo Trainer con los pesos actuales y los nuevos argumentos
trainer_additional_epoch = Trainer(
    model=model,
    args=additional_training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collate_fn,
    compute_metrics=compute_metrics
)

trainer_additional_epoch.train()

ruta_pcam_test = base_folder+"/Datasets/pcamelyon_tf/TFcamelyon_test"
ds_test = tf.data.Dataset.load(ruta_pcam_test)
torch_dataset_test = TfToTorchvisionDataset(ds_test)
ds_test = None

print(f"Número de ejemplos de test: {len(torch_dataset_test)}")

# Evaluar el modelo en el conjunto de test
eval_results = trainer.evaluate(eval_dataset=torch_dataset_test)

print("Resultados de la evaluación en el conjunto de test:")
eval_results

# Guardar el modelo y los pesos
processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
processor.save_pretrained(base_folder+"/Modelos entrenados/google-vit-base v1")
model.save_pretrained(base_folder+"/Modelos entrenados/google-vit-base v1")