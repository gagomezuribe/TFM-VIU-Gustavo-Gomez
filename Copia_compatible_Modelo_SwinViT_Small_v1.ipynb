{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\" MODELO PRE ENTRENADO SwinViT Small\n",
        "Dataset: Patch Camelyon, de tensorflow preparado para HuggingFace\n",
        "Fuente del modelo: HuggingFace\n",
        "Modelo: microsoft/swin-small-patch4-window7-224\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9AobPID6H2m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conectar con Google Drivee()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "base_folder = \"/content/drive/MyDrive/00 VIU/10 TFM\"\n"
      ],
      "metadata": {
        "id": "YA2FG9cPrItg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1532RVbJgQV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# librerías\n",
        "!pip install -q datasets evaluate keras_cv\n",
        "!pip install --upgrade transformers\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tf-keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import transformers\n",
        "import evaluate\n",
        "from transformers import AutoModelForImageClassification, AutoImageProcessor,TrainingArguments, Trainer\n",
        "from datasets import Dataset as HuggingFaceDataset, Features\n",
        "from PIL import Image as PILImage\n",
        "import torch"
      ],
      "metadata": {
        "id": "vlGMxtE4rTdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkpk_JPlCww8"
      },
      "outputs": [],
      "source": [
        "# importar credenciales de Hugging face\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Accede al token almacenado en los secretos de Colab\n",
        "hf_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcE455KaG687"
      },
      "source": [
        "## Cargar el dataset patch camelyon de hugging face"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_hf_ds = HuggingFaceDataset.load_from_disk(base_folder+\"/Datasets/pcamelyon_hf/train 12k\")\n",
        "\n",
        "val_hf_ds = HuggingFaceDataset.load_from_disk(base_folder+\"/Datasets/pcamelyon_hf/val 12k\")"
      ],
      "metadata": {
        "id": "fusjJGwmn2C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxoikSOjs0K"
      },
      "source": [
        "## Preprocesar los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WMEawzyCEyG"
      },
      "outputs": [],
      "source": [
        "modelo = \"microsoft/swin-small-patch4-window7-224\"\n",
        "batch_size = 32\n",
        "\n",
        "# instanciar el image processing\n",
        "image_processor = AutoImageProcessor.from_pretrained(modelo,do_rescale=True,use_fast=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_hf_dataset(examples):\n",
        "    images_pil = [img for img in examples[\"image\"]]\n",
        "    inputs = image_processor(images=images_pil, return_tensors=\"pt\")\n",
        "\n",
        "    # inputs[\"pixel_values\"] es un solo tensor PyTorch de forma (batch_size_interno, C, H, W)\n",
        "    # Debemos dividirlo en una lista de tensores individuales (C, H, W), uno por cada imagen\n",
        "    # para que el Dataset de Hugging Face lo almacene correctamente por ejemplo.\n",
        "    # .unbind(0) divide el tensor a lo largo de la dimensión 0 (batch_size)\n",
        "    pixel_values_list = list(inputs[\"pixel_values\"].unbind(0))\n",
        "\n",
        "    # `examples[\"label\"]` es una lista de Python `int`s cuando `batched=True`\n",
        "    labels_list = examples[\"label\"]\n",
        "\n",
        "    return {\"pixel_values\": pixel_values_list, \"labels\": labels_list}"
      ],
      "metadata": {
        "id": "z3OMOV6L_lDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hf_ds = train_hf_ds.map(preprocess_hf_dataset, batched=True, remove_columns=[\"image\"],load_from_cache_file=False)\n",
        "val_hf_ds = val_hf_ds.map(preprocess_hf_dataset, batched=True, remove_columns=[\"image\"],load_from_cache_file=False)"
      ],
      "metadata": {
        "id": "NTS8BK9JAQJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9DDujL0q1ac"
      },
      "outputs": [],
      "source": [
        "num_labels = 2\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    modelo,\n",
        "    num_labels=num_labels,\n",
        "    ignore_mismatched_sizes = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-QcS9DdtCgG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "id": "UxGAFAHgCdjU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cantidad de parámetros del modelo: \",(sum(p.numel() for p in model.parameters())//1e5)/10, \"M\")  # total parámetros\n"
      ],
      "metadata": {
        "id": "Lt6kg0uwDZhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVWfiBuv2uCS"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    predictions = np.argmax(p.predictions, axis=1)\n",
        "    references = p.label_ids\n",
        "    metrics_result = metric.compute(predictions=predictions, references=references)\n",
        "    print(f\"Metric computation result: {metrics_result}\")\n",
        "    return {\"eval_accuracy\": metrics_result[\"accuracy\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc_MTm0Ks3DF"
      },
      "outputs": [],
      "source": [
        "# establecer los parámetros del entrenamiento\n",
        "args = TrainingArguments(\n",
        "    remove_unused_columns=False,\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    report_to=\"none\",\n",
        "    output_dir=base_folder+\"/Modelos entrenados/Swin v1 Small\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Collator"
      ],
      "metadata": {
        "id": "MRWx-sJI8Xo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0WcwsX7rW9w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def custom_image_collator(batch):\n",
        "    processed_pixel_values_for_batch = []\n",
        "    for example in batch:\n",
        "        current_pixel_values = example[\"pixel_values\"] # Esto es una <class 'list'>\n",
        "        if isinstance(current_pixel_values, list):\n",
        "            temp_tensor = None\n",
        "            if len(current_pixel_values) > 0:\n",
        "                if torch.is_tensor(current_pixel_values[0]):\n",
        "                    if len(current_pixel_values) == 3 and all(torch.is_tensor(x) for x in current_pixel_values):\n",
        "                        temp_tensor = torch.stack(current_pixel_values) # stackea canales: (C, H, W)\n",
        "                    elif len(current_pixel_values) == 1 and torch.is_tensor(current_pixel_values[0]):\n",
        "                        temp_tensor = current_pixel_values[0]\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unexpected list of Tensors structure for pixel_values: {current_pixel_values[0].shape if len(current_pixel_values) > 0 else 'empty list'}\")\n",
        "                elif isinstance(current_pixel_values[0], list):\n",
        "                    temp_tensor = torch.tensor(current_pixel_values, dtype=torch.float32) # Convertir la lista anidada a tensor\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected element type in pixel_values list: {type(current_pixel_values[0])}. Expected Tensor or list.\")\n",
        "            else:\n",
        "                raise ValueError(\"pixel_values list is empty.\")\n",
        "        elif torch.is_tensor(current_pixel_values):\n",
        "            temp_tensor = current_pixel_values\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected main type for pixel_values: {type(current_pixel_values)}. Expected list or Tensor.\")\n",
        "\n",
        "        processed_pixel_values_for_batch.append(temp_tensor)\n",
        "\n",
        "    stacked_pixel_values = torch.stack(processed_pixel_values_for_batch)\n",
        "\n",
        "\n",
        "    labels_clean = []\n",
        "    for example in batch:\n",
        "        if isinstance(example[\"labels\"], torch.Tensor):\n",
        "            labels_clean.append(example[\"labels\"].item() if example[\"labels\"].numel() == 1 else example[\"labels\"].tolist()[0])\n",
        "        else:\n",
        "            labels_clean.append(example[\"labels\"])\n",
        "    stacked_labels = torch.tensor(labels_clean, dtype=torch.long)\n",
        "\n",
        "    return {\"pixel_values\": stacked_pixel_values, \"labels\": stacked_labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McVoaCPr3Cj-"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_hf_ds,\n",
        "    eval_dataset=val_hf_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=custom_image_collator,\n",
        "    tokenizer=image_processor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pps61vF_4QaH"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "print(\"\\n--- Iniciando entrenamiento ---\")\n",
        "train_results = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(base_folder+\"/Modelos entrenados/Swin Small v1\")"
      ],
      "metadata": {
        "id": "ypjTY52A2H4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUAR EL MODELO\n",
        "\n",
        "# Cargar el dataset de test 32k\n",
        "test_hf_ds = HuggingFaceDataset.load_from_disk(base_folder+\"/Datasets/pcamelyon_hf/test\")\n",
        "\n",
        "# Preprocesar el dataset de test\n",
        "test_hf_ds = test_hf_ds.map(preprocess_hf_dataset, batched=True, remove_columns=[\"image\"], load_from_cache_file=False)\n",
        "\n",
        "# Evaluar el modelo con el dataset de test\n"
      ],
      "metadata": {
        "id": "VhMiqDv1z5Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "test_results = trainer.evaluate(test_hf_ds)\n",
        "\n",
        "print(\"\\n--- Resultados de la evaluación en el dataset de test ---\")\n",
        "print(test_results)\n"
      ],
      "metadata": {
        "id": "sOeOkuH-0vOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0nRTVUj00fj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params}\")"
      ],
      "metadata": {
        "id": "ZP2yn7mu1uQh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}